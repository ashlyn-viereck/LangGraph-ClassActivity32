{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJygm2q2Opa6",
        "outputId": "b969da3d-42cb-4917-b6f3-811dbdd7383e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-1.0.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain\n",
            "  Downloading langchain-1.0.8-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.0.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.79)\n",
            "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.11.10)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Collecting langchain-core>=0.1 (from langgraph)\n",
            "  Downloading langchain_core-1.0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.4.42)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
            "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.5.0)\n",
            "Downloading langgraph-1.0.3-py3-none-any.whl (156 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-1.0.8-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.7/93.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-1.0.3-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.0.7-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.0/473.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\n",
            "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langchain-core, langgraph-checkpoint, langchain-openai, langgraph-prebuilt, langgraph, langchain\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.79\n",
            "    Uninstalling langchain-core-0.3.79:\n",
            "      Successfully uninstalled langchain-core-0.3.79\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.27\n",
            "    Uninstalling langchain-0.3.27:\n",
            "      Successfully uninstalled langchain-0.3.27\n",
            "Successfully installed langchain-1.0.8 langchain-core-1.0.7 langchain-openai-1.0.3 langgraph-1.0.3 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.2.9 ormsgpack-1.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langgraph langchain langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The API Key should be named \"OPENAI_API_KEY\", while creating the key (https://openrouter.ai/settings/keys)"
      ],
      "metadata": {
        "id": "loRR4XgQl9p5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"INSERT_API_KEY_HERE\"\n",
        "\n",
        "#INSERT KEY HERE\n"
      ],
      "metadata": {
        "id": "T_jUERbdOy2I"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OGh_7lBPI9T",
        "outputId": "fa8eee22-a4ac-4ff8-95e9-0e0f0d8b361b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        ")"
      ],
      "metadata": {
        "id": "umSRm4eSPTF_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "\n",
        "class CounterState(TypedDict):\n",
        "    count: int"
      ],
      "metadata": {
        "id": "UKHDJtQXPhKi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def increment(state: CounterState) -> dict:\n",
        "    state[\"count\"] += 1\n",
        "    return state"
      ],
      "metadata": {
        "id": "KGI7eE8tPmxf"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "builder = StateGraph(CounterState)\n",
        "\n",
        "builder.add_node(\"increment\", increment)\n",
        "\n",
        "# Define the execution order: START -> increment -> END\n",
        "builder.add_edge(START, \"increment\")\n",
        "builder.add_edge(\"increment\", END)\n",
        "\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "id": "_9R37jSsPqRL"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_state: CounterState = {\"count\": 0}\n",
        "\n",
        "result = graph.invoke(initial_state)\n",
        "\n",
        "print(result)\n",
        "\n",
        "# Output\n",
        "# {'count': 1}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMCgWsGdPtBK",
        "outputId": "bb132723-f3a9-4765-ac4c-51c4e4347651"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'count': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "\n",
        "def should_continue(state: CounterState) -> Literal[\"increment\", END]:\n",
        "\n",
        "    if state[\"count\"] <= 3: # keep looping\n",
        "      #print(\"count:\", count)\n",
        "      return \"increment\"\n",
        "\n",
        "\n",
        "    return END # stop the graph"
      ],
      "metadata": {
        "id": "GJkyLLbePxIY"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder = StateGraph(CounterState)\n",
        "\n",
        "builder.add_node(\"increment\", increment)\n",
        "\n",
        "builder.add_edge(START, \"increment\")\n",
        "\n",
        "builder.add_conditional_edges(\n",
        "    \"increment\",\n",
        "    should_continue,\n",
        "    [\"increment\", END],\n",
        ")\n",
        "\n",
        "graph = builder.compile()\n",
        "\n",
        "result = graph.invoke({\"count\": 0})\n",
        "\n",
        "print(result)\n",
        "\n",
        "# Output\n",
        "# {'count': 3}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfwAmGYoP1Se",
        "outputId": "c191b8b2-5c01-4c06-b249-3e2fb105a81c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'count': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    user_input: str\n",
        "    response: str"
      ],
      "metadata": {
        "id": "NaZ1IA8PROO4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        ")"
      ],
      "metadata": {
        "id": "tREjN5UrRPaA"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "def llm_node(state: AgentState) -> dict:\n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "        HumanMessage(content=state[\"user_input\"]),\n",
        "    ]\n",
        "\n",
        "    reply = llm.invoke(messages)\n",
        "\n",
        "    return {\"response\": reply.content}"
      ],
      "metadata": {
        "id": "YST5401VRRs-"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "builder = StateGraph(AgentState)\n",
        "\n",
        "builder.add_node(\"llm\", llm_node)\n",
        "\n",
        "# define the flow: START -> llm -> END\n",
        "builder.add_edge(START, \"llm\")\n",
        "builder.add_edge(\"llm\", END)\n",
        "\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "id": "ALUkfCOiRVn6"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_state: AgentState = {\n",
        "    \"user_input\": \"Tell me about Ashlyn Viereck at Creighton University?\",\n",
        "    \"response\": \"\",\n",
        "}\n",
        "\n",
        "result = graph.invoke(initial_state)\n",
        "\n",
        "print(\"User:\", initial_state[\"user_input\"])\n",
        "print(\"Assistant:\", result[\"response\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GykLlHBdRiWK",
        "outputId": "2a1315e9-82d3-4560-df67-0555b0293f51"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Tell me about Ashlyn Viereck at Creighton University?\n",
            "Assistant: As of my last knowledge update in October 2021, I don’t have specific information on an individual named Ashlyn Viereck at Creighton University. It's possible that she is a student, faculty member, or involved in some capacity at the university, but there are no publicly available details in my training data regarding her.\n",
            "\n",
            "For the most accurate and current information, I recommend visiting Creighton University's official website or contacting the university directly. You might also check social media platforms or news releases that could feature updates about students or staff.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state2: AgentState = {\n",
        "    \"user_input\": \"Can you summarise that in two lines?\",\n",
        "    \"response\": \"\",\n",
        "}\n",
        "result2 = graph.invoke(state2)\n",
        "\n",
        "print(\"\\nTurn 2 - User:\", state2[\"user_input\"])\n",
        "print(\"Turn 2 - Assistant:\", result2[\"response\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kvy6H7fyY5-X",
        "outputId": "c81f9f1a-4f2f-466d-e2ba-68e1aa61e8f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Turn 2 - User: Can you summarise that in two lines?\n",
            "Turn 2 - Assistant: Of course! However, I need the content you want summarized. Please provide the text or main ideas you'd like to condense into two lines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict, Annotated\n",
        "from operator import add\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "class State(TypedDict):\n",
        "    foo: str\n",
        "    bar: Annotated[list[str], add]\n",
        "\n",
        "def node_a(state: State):\n",
        "    # overwrite foo, append \"a\" to bar\n",
        "    return {\"foo\": \"a\", \"bar\": [\"a\"]}\n",
        "\n",
        "def node_b(state: State):\n",
        "    # overwrite foo, append \"b\" to bar\n",
        "    return {\"foo\": \"b\", \"bar\": [\"b\"]}"
      ],
      "metadata": {
        "id": "8thKkKNza46n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder = StateGraph(State)\n",
        "builder.add_node(\"node_a\", node_a)\n",
        "builder.add_node(\"node_b\", node_b)\n",
        "\n",
        "builder.add_edge(START, \"node_a\")\n",
        "builder.add_edge(\"node_a\", \"node_b\")\n",
        "builder.add_edge(\"node_b\", END)\n",
        "\n",
        "checkpointer = InMemorySaver()\n",
        "graph = builder.compile(checkpointer=checkpointer)"
      ],
      "metadata": {
        "id": "S5gy2b0lbLfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "final_state = graph.invoke({\"foo\": \"\", \"bar\": []}, config=config)\n",
        "print(\"Final state:\", final_state)\n",
        "\n",
        "# Output\n",
        "# Final state: {'foo': 'b', 'bar': ['a', 'b']}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmLXOoLTbMf5",
        "outputId": "8907f989-bfb4-496c-c3e5-f46ea70e083f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final state: {'foo': 'b', 'bar': ['a', 'b']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = list(graph.get_state_history(config))\n",
        "\n",
        "for i, snap in enumerate(history[::-1]):\n",
        "    print(f\"\\nCheckpoint {i}:\")\n",
        "    print(\"  created_at:\", snap.created_at)\n",
        "    print(\"  node:\", snap.metadata)\n",
        "    print(\"  values:\", snap.values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELtyVBjxbQZY",
        "outputId": "2be487b7-e5c3-49c5-baf3-4e8988643153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint 0:\n",
            "  created_at: 2025-11-18T20:29:25.831759+00:00\n",
            "  node: {'source': 'input', 'step': -1, 'parents': {}}\n",
            "  values: {'bar': []}\n",
            "\n",
            "Checkpoint 1:\n",
            "  created_at: 2025-11-18T20:29:25.833885+00:00\n",
            "  node: {'source': 'loop', 'step': 0, 'parents': {}}\n",
            "  values: {'foo': '', 'bar': []}\n",
            "\n",
            "Checkpoint 2:\n",
            "  created_at: 2025-11-18T20:29:25.835782+00:00\n",
            "  node: {'source': 'loop', 'step': 1, 'parents': {}}\n",
            "  values: {'foo': 'a', 'bar': ['a']}\n",
            "\n",
            "Checkpoint 3:\n",
            "  created_at: 2025-11-18T20:29:25.836613+00:00\n",
            "  node: {'source': 'loop', 'step': 2, 'parents': {}}\n",
            "  values: {'foo': 'b', 'bar': ['a', 'b']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict, Annotated\n",
        "from langchain_core.messages import AnyMessage\n",
        "import operator\n",
        "\n",
        "class MessagesState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], operator.add]"
      ],
      "metadata": {
        "id": "HPRqRWNBbWuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "def chat_llm_node(state: MessagesState) -> dict:\n",
        "\n",
        "    # build prompt from system + existing conversation\n",
        "    history = [SystemMessage(content=\"You are a helpful assistant.\")]\n",
        "    history.extend(state[\"messages\"])\n",
        "\n",
        "    reply = llm.invoke(history)\n",
        "\n",
        "    return {\"messages\": [reply]}"
      ],
      "metadata": {
        "id": "3QsB8Jb3bbvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "checkpointer = InMemorySaver()\n",
        "\n",
        "builder = StateGraph(MessagesState)\n",
        "builder.add_node(\"chat_llm\", chat_llm_node)\n",
        "builder.add_edge(START, \"chat_llm\")\n",
        "builder.add_edge(\"chat_llm\", END)\n",
        "\n",
        "graph = builder.compile(checkpointer=checkpointer)"
      ],
      "metadata": {
        "id": "QPLmmTf_bdUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"user_123\"}}\n"
      ],
      "metadata": {
        "id": "5wf5Dh1AbgX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"user_123\"}}\n",
        "\n",
        "# Turn 1\n",
        "state1 = {\"messages\": [HumanMessage(content=\"What is GIL in python?\")]}\n",
        "result1 = graph.invoke(state1, config=config)\n",
        "\n",
        "for m in result1[\"messages\"]:\n",
        "    print(type(m).__name__, \":\", m.content)\n",
        "\n",
        "# Turn 2\n",
        "state2 = {\"messages\": [HumanMessage(content=\"Summarise it in two lines\")],}\n",
        "result2 = graph.invoke(state2, config=config)\n",
        "\n",
        "for m in result2[\"messages\"][-2:]:\n",
        "    print(type(m).__name__, \":\", m.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhgqZdMmbjL1",
        "outputId": "5cb3e07a-4565-4614-d8f2-a9068ddd217d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HumanMessage : What is GIL in python?\n",
            "AIMessage : The Global Interpreter Lock (GIL) is a mechanism used in the CPython implementation of Python to ensure that only one thread executes Python bytecode at a time. This means that even though Python supports multi-threading, the GIL can prevent true parallel execution of threads in a single process. \n",
            "\n",
            "Here are some key points about the GIL:\n",
            "\n",
            "1. **Purpose**: The GIL simplifies memory management and protects access to Python objects, making it easier to maintain internal state while allowing threads to share the same interpreter.\n",
            "\n",
            "2. **Impact on Multi-threading**: Because of the GIL, CPU-bound multi-threaded programs might not see a performance improvement because the GIL allows only one thread to execute at a time. However, I/O-bound thread programs can benefit from multi-threading, as the GIL is released while waiting for I/O operations to complete.\n",
            "\n",
            "3. **Workarounds**: To achieve true parallelism in CPU-bound tasks, Python developers can use multi-processing (using the `multiprocessing` module), which creates separate processes each with its own Python interpreter and memory space, bypassing the GIL.\n",
            "\n",
            "4. **Other Implementations**: It's worth noting that not all Python implementations have a GIL. For example, Jython (Python on the Java platform) and IronPython (Python on the .NET framework) do not have a GIL and can achieve true parallelism.\n",
            "\n",
            "The GIL is a subject of much discussion and debate in the Python community because it impacts the performance characteristics of multi-threaded Python applications.\n",
            "HumanMessage : Summarise it in two lines\n",
            "AIMessage : The Global Interpreter Lock (GIL) in Python ensures that only one thread executes Python bytecode at a time, simplifying memory management but limiting true parallel execution in CPU-bound programs. To achieve parallelism, developers can use multi-processing instead of multi-threading.\n"
          ]
        }
      ]
    }
  ]
}